# lightrainer
Simple Trainer for PyTorch Deep Learning

## ğŸ“˜ æ–‡æ¡£

<details open>
<summary>å®‰è£…</summary>

å°†ä»“åº“å…‹éš†å¹¶åœ¨ä¸€ä¸ª[**Python â‰¥ 3.12.0**](https://www.python.org/)çš„ç¯å¢ƒä¸­å®‰è£…ä¾èµ–é¡¹ã€‚è¯·ç¡®ä¿ä½ å·²å®‰è£…[**PyTorch â‰¥ 2.7.0+cu128**](https://pytorch.org/get-started/locally/) ã€‚

```bash
git clone https://github.com/fordelkon/lightrainer.git

cd lightrainer

pip install -r requirements.txt
```

<details open>
<summary>ä½¿ç”¨æ•™ç¨‹</summary>

- **åŸºç±»è®­ç»ƒå™¨ï¼ˆBaseTranerï¼‰**
    - åŸºç±»è®­ç»ƒå™¨æä¾›è®­ç»ƒæ‰€éœ€è¦çš„åŸºæœ¬æ­¥éª¤: `train`ï¼Œ`validate`ï¼Œ`get_loss`ï¼Œ`get_loader`ï¼Œ`save_model` ç­‰æ–¹æ³•éœ€è¦æˆ‘ä»¬åœ¨ä¸åŒä»»åŠ¡ä¸­è‡ªå®šä¹‰éœ€æ±‚ï¼Œå…¶ä½™æ–¹æ³•æ¯”å¦‚ä¸€äº›æ˜¯æ¨¡å‹é‡åŒ–æ‰€éœ€è¦çš„æ–¹æ³•`replace_to_quant_modules`ã€`collect_stats`å’Œ`compute_max`ï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ— éœ€æ”¹åŠ¨çš„é…ç½®æ–¹æ³•ä»¥åŠä¼˜åŒ–æ–¹æ³•å¦‚`setup_train`ã€`build_optimizer`ã€`setup_scheduler`ã€`optimizer_step`ã€`get_memory`ã€`clear_memory`ï¼Œæœ€åä¸€äº›æ˜¯è®­ç»ƒåçš„ä»¥ä¸åŒçš„æ–‡ä»¶æ ¼å¼å¯¼å‡ºå¦‚`export_to_onnx`å’Œ`export_to_tensorrt`ã€‚

- **è®­ç»ƒå‚æ•°é…ç½®ï¼ˆYaMLæ–‡ä»¶ï¼‰**
    - è®­ç»ƒå‚æ•°å¯ä»¥é€šè¿‡è®¾è®¡ä¸åŒçš„`.yml`æ–‡ä»¶æ¥è¿›è¡Œé…ç½®ï¼Œæœ€åŸºæœ¬çš„å¯è®¾å®šå‚æ•°å¦‚ä¸‹
    ```yaml
    save_dir: runs_eeg  # (str) ç”¨äºä¿å­˜å®éªŒç»“æœçš„ç›®å½•
    device: 0           # (int) ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼Œä¾‹å¦‚ 0ã€1 æˆ– 2ï¼Œè¡¨ç¤ºä½¿ç”¨å“ªå— GPU
    batch: 32           # (int) æ‰¹é‡å¤§å°
    nbs: 64             # (int) æ ‡å‡†æ‰¹é‡å¤§å°ï¼ˆç”¨äºå½’ä¸€åŒ–å­¦ä¹ ç‡ï¼‰
    num_workers: 4      # (int) ç”¨äºå¹¶è¡ŒåŠ è½½æ•°æ®çš„å­è¿›ç¨‹æ•°é‡
    patience: 100       # (int) åœ¨æ²¡æœ‰æå‡çš„æƒ…å†µä¸‹ï¼Œæœ€å¤šç­‰å¾…å¤šå°‘ä¸ª epoch ååœæ­¢è®­ç»ƒ
    epochs: 200         # (int) æ€»è®­ç»ƒè½®æ•°
    exist_ok: True      # (bool) æ˜¯å¦å…è®¸è¦†ç›–å·²å­˜åœ¨çš„å®éªŒç›®å½•
    amp: False          # (bool) æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
    ptq: False          # (bool) æ˜¯å¦ä½¿ç”¨ NVIDIA çš„ pytorch_quantization è¿›è¡Œè®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰
    num_batches: None   # (None æˆ– int) åœ¨ PTQ ä¸­ï¼Œç”¨äºæ”¶é›†ç»Ÿè®¡ä¿¡æ¯çš„æ‰¹æ¬¡æ•°é‡
    method: entropy     # (str) åœ¨ PTQ ä¸­ç”¨äºå†³å®šæœ€å¤§ç»å¯¹å€¼çš„æ–¹æ³•ï¼Œå¯é€‰å€¼=[entropy, max, percentile, mse]
    qat: False          # (bool) æ˜¯å¦ä½¿ç”¨ NVIDIA çš„ pytorch_quantization è¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰
    trt_quant: False    # (bool) æ˜¯å¦ä½¿ç”¨ TensorRT çš„ int8 é‡åŒ–

    optimizer: Adam     # (str) ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼Œå¯é€‰å€¼=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp]
    scheduler: Cosine   # (str) å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œå¯é€‰å€¼=[Cosine, Linear, Exponential, Polynomial]
    lr0: 0.01           # (float) åˆå§‹å­¦ä¹ ç‡ï¼ˆå¦‚ SGD=1E-2ï¼ŒAdam=1E-3ï¼‰
    lrf: 0.01           # (float) æœ€ç»ˆå­¦ä¹ ç‡ä¸ºåˆå§‹å­¦ä¹ ç‡ä¹˜ä»¥è¯¥å€¼
    gamma: 0.9          # (float) < 1.0 æ—¶ç”¨äºæŒ‡æ•°è¡°å‡çš„å› å­ï¼ˆæ¯è½®å­¦ä¹ ç‡ä¹˜ä»¥ gammaï¼‰
    power: 2.0          # (float) ç”¨äºå¤šé¡¹å¼è°ƒåº¦çš„æ›²çº¿å½¢çŠ¶æ§åˆ¶å› å­
    momentum: 0.937     # (float) SGD çš„åŠ¨é‡/Adam çš„ beta1 å‚æ•°
    weight_decay: 0.0005 # (float) ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡å‚æ•°ï¼Œå…¸å‹å€¼ä¸º 5e-4

    warmup_epochs: 3.0       # (float) é¢„çƒ­è®­ç»ƒçš„ epoch æ•°ï¼ˆæ”¯æŒå°æ•°ï¼‰
    warmup_momentum: 0.8     # (float) é¢„çƒ­é˜¶æ®µçš„åˆå§‹åŠ¨é‡
    warmup_bias_lr: 0.1      # (float) é¢„çƒ­é˜¶æ®µåç½®é¡¹çš„åˆå§‹å­¦ä¹ ç‡
    ```
    ä½†æ˜¯æˆ‘ä»¬ä»ç„¶å¯ä»¥ç»§ç»­åœ¨ä¸Šè¿°åŸºæœ¬é…ç½®å‚æ•°ä¸­ç»§ç»­æ·»åŠ ä¸€äº›ç‰¹å®šçš„å‚æ•°ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚æ¯”å¦‚è¯´åœ¨`VAVQE`çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸Šè¿°å‚æ•°æ¡ä»¶ä¸‹æ·»åŠ ä»¥ä¸‹é¢å¤–å‚æ•°
    ```yaml
    recon_weight: 10.0
    lpips_weight: 1.0
    vq_weight: 0.5
    ```
    è¡¨ç¤ºå¯¹äºä¸åŒæŸå¤±æ‰€èµ‹äºˆçš„æƒé‡ï¼Œåœ¨è‡ªå®šä¹‰ä»»åŠ¡çš„è®­ç»ƒå™¨ä¸­è°ƒç”¨`self.kwdict[key]`æ¥è¿›è¡Œä½¿ç”¨ã€‚

- **åˆ†ç±»è®­ç»ƒå™¨ï¼ˆCLSTrainerï¼‰**
    - è‡ªå®šä¹‰åˆ†ç±»è®­ç»ƒå™¨ä¸­çš„è®­ç»ƒå‡½æ•°
    ```python 
    def train(self, use_wandb: bool=False):
        self.use_wandb = self.use_wandb & use_wandb
        self.setup_train()
        if self.use_wandb:
            wandb.init(
                project="cls-task", 
                config={
                    "batch_size": self.batch_size,
                    "initial_learning_rate": self.lr0, 
                    "final_learning_rate": self.lr0 * self.scheduler_params["lrf"], 
                    "optimizer": self.optimizer_choice, 
                    "scheduler": self.scheduler_choice, 
                    "weight_decay": self.weight_decay, 
                    "warmup_epochs": self.warmup_epochs, 
                    "warmup_momentum": self.warmup_momentum, 
                    "warmup_bias_lr": self.warmup_bias_lr, 
                }
            )
        self.model = self.model.to(self.device)

        nb = len(self.train_loader) # number of batches
        nw = max(round(self.warmup_epochs * nb), 100) if self.warmup_epochs > 0 else -1 # warmup iterations
        last_opt_step = -1
        self.optimizer.zero_grad()
        for epoch in range(1, self.epochs + 1):
            print("\n")
            print("-" * 30)
            print("\n")
            self.train_loss = 0.
            self.start_epoch = epoch
            with warnings.catch_warnings():
                warnings.simplefilter("ignore") # supress 'Detected lr_scheduler.step() before optimizer.step()'
                self.scheduler.step() # start schuduler <-> `last_epoch=0``
            
            self.model.train() # No freeze layers
            pbar = enumerate(self.train_loader)
            
            pbar = tqdm(enumerate(self.train_loader), total=nb)
            
            for i, batch in pbar:
                # Warmup 
                ni = i + nb * epoch
                if ni <= nw:
                    xi = [0, nw] # x interp
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.nbs / self.batch_size]).round()))
                    for j, x in enumerate(self.optimizer.param_groups):
                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                        x["lr"] = np.interp(
                            ni, xi, [self.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lr_func(epoch)]
                        )
                        if "momentum" in x:
                            x["momentum"] = np.interp(ni, xi, [self.warmup_momentum, self.momentum])

                # Forward
                _, loss = self.get_loss(batch)
                self.train_loss += loss.item()
                
                # Backward
                self.scaler.scale(loss).backward()

                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()
                    last_opt_step = ni

                pbar.set_description(
                    f"{epoch}/{self.epochs}",
                    f"{self.get_memory():.3g}G",  # (GB) GPU memory util
                )

            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers
            print(f"All types `lr` of epoch {epoch}: {self.lr}")

            self.train_loss /= len(self.train_loader)
            print(f"Epoch {epoch}: Train loss {self.train_loss}")
            if self.use_wandb:
                wandb.log(
                    {
                        "epoch": epoch, 
                        "train/avg_loss": self.train_loss, 
                    }
                )

            final_epoch = epoch + 1 >= self.epochs

            self.val_loss, self.fitness = self.validate()
            print(f"Epoch {epoch}: Val loss {self.val_loss}, Val Acc {self.fitness}")
            print(f"Epoch {epoch} Val Acc: {self.fitness}, before best Acc: {self.stopper.best_fitness}")
            self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch
            self.best_fitness = self.stopper.best_fitness

            if self.stop:
                break

            if not final_epoch and not self.qat:
                self.save_dir = increment_path(Path(self.kwdict["save_dir"]), exist_ok=self.kwdict["exist_ok"])  # increment save_dir
                self.last, self.best = self.save_dir / "last.pt", self.save_dir / "best.pt"
                
                self.save_model()

            if self.get_memory(fraction=True) > 0.5:
                self.clear_memory() # clear if memory utilization > 50%

            print(f"\n{self.start_epoch} epochs completed!")
            print("\n")
            print("-" * 30)
            print("\n")
        self.clear_memory()
        if self.use_wandb:
            wandb.finish()

        # After train we can export onnx
        self.onnx = self.save_dir / "best.onnx"

    @torch.inference_mode()
    def validate(self):
        self.model.eval()
        pbar = enumerate(self.val_loader)
        pbar = tqdm(pbar, total=len(self.val_loader), bar_format="{l_bar}{bar:10}{r_bar}")
        y_true, y_pred = [], []

        val_loss = 0.0
        for _, batch in pbar:
            X, y = batch
            logits, loss = self.get_loss(batch)
            val_loss += loss.item()
            pred = logits.argmax(dim=1)
            y_true.append(y.cpu().numpy())
            y_pred.append(pred.cpu().numpy())
        
        val_loss /= len(self.val_loader)
        if self.use_wandb:
            wandb.log(
                {
                    "epoch": self.start_epoch,
                    "val/avg_loss": val_loss, 
                }
            )
        y_true = np.array(y_true).reshape(-1)
        y_pred = np.array(y_pred).reshape(-1)

        return val_loss, sklearn.metrics.accuracy_score(y_true, y_pred)
    
    def get_loss(self, batch):
        X, y = batch[0], batch[1]
        X, y = X.to(self.device), y.to(self.device)
        logits = self.model(X)
        loss = nn.functional.cross_entropy(logits, y) 
        return logits, loss
    ```

- **å›¾åƒç”Ÿæˆè®­ç»ƒå™¨ï¼ˆImGenTrainerï¼‰**
    - è‡ªå®šä¹‰å›¾åƒç”Ÿæˆè®­ç»ƒå™¨çš„è®­ç»ƒå‡½æ•°ï¼ˆé’ˆå¯¹`VQVAE`ï¼‰
    ```python
    def train(self, use_wandb: bool=False):
        self.use_wandb = use_wandb & self.use_wandb
        self.setup_train()
        if self.use_wandb:
            wandb.init(
                project="imgen-task", 
                config={
                    "batch_size": self.batch_size,
                    "initial_learning_rate": self.lr0, 
                    "final_learning_rate": self.lr0 * self.scheduler_params["lrf"], 
                    "optimizer": self.optimizer_choice, 
                    "scheduler": self.scheduler_choice, 
                    "weight_decay": self.weight_decay, 
                    "warmup_epochs": self.warmup_epochs, 
                    "warmup_momentum": self.warmup_momentum, 
                    "warmup_bias_lr": self.warmup_bias_lr, 
                    "recon_weight": self.kwdict["recon_weight"], 
                    "lpips_weight": self.kwdict["lpips_weight"], 
                    "vq_weight": self.kwdict["vq_weight"],
                }
            )
        self.lpips_fn = lpips.LPIPS().to(self.device)
        self.model = self.model.to(self.device)

        nb = len(self.train_loader) # number of batches
        nw = max(round(self.warmup_epochs * nb), 100) if self.warmup_epochs > 0 else -1 # warmup iterations
        last_opt_step = -1
        self.optimizer.zero_grad()
        for epoch in range(1, self.epochs + 1):
            print("\n")
            print("-" * 30)
            print("\n")
            self.train_loss = 0.
            self.train_loss_recon = 0.
            self.train_loss_lpips = 0.
            self.train_loss_vq = 0.
            self.train_perplexity = 0.
            self.start_epoch = epoch
            with warnings.catch_warnings():
                warnings.simplefilter("ignore") # supress 'Detected lr_scheduler.step() before optimizer.step()'
                self.scheduler.step()
            
            self.model.train() # No freeze layers
            pbar = enumerate(self.train_loader)
            
            pbar = tqdm(enumerate(self.train_loader), total=nb)
            
            for i, batch in pbar:
                # Warmup 
                ni = i + nb * epoch
                if ni <= nw:
                    xi = [0, nw] # x interp
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.nbs / self.batch_size]).round()))
                    for j, x in enumerate(self.optimizer.param_groups):
                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                        x["lr"] = np.interp(
                            ni, xi, [self.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lr_func(epoch)]
                        )
                        if "momentum" in x:
                            x["momentum"] = np.interp(ni, xi, [self.warmup_momentum, self.momentum])

                # Forward
                loss_vq, loss_recon, loss_lpips, perplexity, psnr, ssim = self.get_loss(batch)
                loss = self.kwdict["vq_weight"] * loss_vq + self.kwdict["recon_weight"] * loss_recon + self.kwdict["lpips_weight"] * loss_lpips
                self.train_loss += loss.item()
                self.train_loss_recon += loss_recon.item()
                self.train_loss_lpips += loss_lpips.item()
                self.train_loss_vq += loss_vq.item()
                self.train_perplexity += perplexity.item()
                
                # Backward
                self.scaler.scale(loss).backward()

                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()
                    last_opt_step = ni

                pbar.set_description(
                    f"{epoch}/{self.epochs}",
                    f"{self.get_memory():.3g}G",  # (GB) GPU memory util
                )

                if self.use_wandb:
                    if i % 50 == 0:
                        wandb.log(
                            {
                                "step": (epoch - 1) * nb + i, 
                                "train/loss": loss.item(), 
                                "train/recon_loss": loss_recon.item(), 
                                "train/perceptual_loss": loss_lpips.item(), 
                                "train/vq_loss": loss_vq.item(), 
                                "train/perplexity": perplexity.item(), 
                                "train/psnr": psnr.item(), 
                                "train/ssim": ssim.item(), 
                            }
                        )

            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers
            print(f"All types `lr` of epoch {epoch}: {self.lr}")

            self.train_loss /= nb
            self.train_loss_recon /= nb
            self.train_loss_lpips /= nb
            self.train_loss_vq /= nb
            self.train_perplexity /= nb

            print(f"Epoch {epoch}: Train loss {self.train_loss}")
            if self.use_wandb:
                wandb.log(
                    {
                        "epoch": epoch, 
                        "train/avg_loss": self.train_loss, 
                        "train/avg_recon_loss": self.train_loss_recon, 
                        "train/avg_perceptual_loss": self.train_loss_lpips, 
                        "train/avg_perplexity": self.train_perplexity, 
                    }
                )

            final_epoch = epoch + 1 >= self.epochs

            self.val_loss, perplexity, self.fitness, ssim= self.validate()

            print(f"Epoch {epoch}: Val loss {self.val_loss}, Val Peak Singal-to-Ratio is {self.fitness}, Val Perplexity is {perplexity}, Val Structure Similarity Index is {ssim}.")
            print(f"Epoch {epoch} Val Peak Singal-to-Ratio: {self.fitness}, before best Peak Singal-to-Ratio: {self.stopper.best_fitness}")
            self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch
            self.best_fitness = self.stopper.best_fitness

            if self.stop:
                break

            if not final_epoch and not self.qat:
                self.save_dir = increment_path(Path(self.kwdict["save_dir"]), exist_ok=self.kwdict["exist_ok"])  # increment save_dir
                self.last, self.best = self.save_dir / "last.pt", self.save_dir / "best.pt"
                
                self.save_model()

            if self.get_memory(fraction=True) > 0.5:
                self.clear_memory() # clear if memory utilization > 50%

            print(f"\n{self.start_epoch} epochs completed!")
            print("\n")
            print("-" * 30)
            print("\n")

        self.clear_memory()

        if self.use_wandb:
            wandb.finish()

        # After train we can export onnx
        self.onnx = self.save_dir / "best.onnx"

    @torch.inference_mode()
    def validate(self):
        self.model.eval()
        pbar = enumerate(self.val_loader)
        pbar = tqdm(pbar, total=len(self.val_loader), bar_format="{l_bar}{bar:10}{r_bar}")

        val_loss = 0.
        val_loss_recon = 0.
        val_loss_lpips = 0.
        val_loss_vq = 0.
        val_perplexity = 0.
        val_ssim = 0.
        val_psnr = 0.
        nb_val = len(self.val_loader)
        for _, batch in pbar:
            loss_vq, loss_recon, loss_lpips, perplexity, psnr, ssim = self.get_loss(batch)
            loss = self.kwdict["vq_weight"] * loss_vq + self.kwdict["recon_weight"] * loss_recon + self.kwdict["lpips_weight"] * loss_lpips
            val_loss += loss.item()
            val_loss_recon += loss_recon.item()
            val_loss_lpips += loss_lpips.item()
            val_loss_vq += loss_vq.item()
            val_perplexity += perplexity.item()
            val_ssim += ssim
            val_psnr += psnr
        val_loss /= nb_val
        val_loss_recon /= nb_val
        val_loss_lpips /= nb_val
        val_loss_vq /= nb_val
        val_perplexity /= nb_val
        val_ssim /= nb_val
        val_psnr /= nb_val

        if self.use_wandb:
            wandb.log(
                {
                    "epoch": self.start_epoch,
                    "val/avg_loss": val_loss, 
                    "val/avg_recon_loss": val_loss_recon, 
                    "val/avg_perceptual_loss": val_loss_lpips, 
                    "val/avg_vq_loss": val_loss_vq, 
                    "val/avg_psnr": val_psnr, 
                    "val/avg_ssim": val_ssim, 
                    "val/avg_perplexity": val_perplexity,
                }
            )

        return val_loss, val_perplexity, val_psnr, val_ssim
    
    def get_loss(self, batch):
        X = batch[0]
        X = X.to(self.device)
        X_recon, loss_vq, perplexity = self.model(X)
        # Reconstruction Loss
        loss_recon = nn.functional.mse_loss(X_recon, X)
        # LPIS Loss
        X_norm, X_recon_norm = X * 2. - 1., X_recon * 2. - 1.
        loss_lpips = self.lpips_fn(X_norm, X_recon_norm).mean()

        psnr = kornia.metrics.psnr(X, X_recon, max_val=1.)
        
        ssim = kornia.metrics.ssim(X, X_recon, window_size=11).mean()
    
        return (loss_vq, loss_recon, loss_lpips, 
                perplexity, psnr, ssim)
    ```

<details open>
<summary>æ€»ç»“</summary>

å¯¹æ¯”ä¸¤ä¸ªç‰¹å®šä»»åŠ¡çš„`train`æ–¹æ³•å¯ä»¥çœ‹å‡ºï¼Œè¯¥æ–¹æ³•çš„åŸºæœ¬æ¡†æ¶æ˜¯æ²¡æœ‰ä»€ä¹ˆå˜åŒ–çš„ï¼Œä¸»è¦å˜åŒ–çš„æ˜¯`get_loss`å’Œ`validate`æ–¹æ³•çš„è¿”å›ç»“æœï¼Œç”±äºä¸åŒæ¨¡å‹çš„æŸå¤±ä»¥åŠé€‰æ‹©ä¿å­˜æ¨¡å‹çš„è¯„åˆ¤æ ‡å‡†ä¸åŒï¼Œæ‰€ä»¥è¿™ä¸¤ä¸ªæ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§ã€‚å½“ç„¶æœ‰äº›æ—¶å€™éœ€è¦å¯¹`dataset`è¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†æ¯”å¦‚`padding`ç­‰æ–¹æ³•ï¼Œéœ€è¦å†å®šä¹‰`collate_fn`æ–¹æ³•å†`get_loader`é‡Œä½¿ç”¨ã€‚æœ‰äº›æƒ…å†µæˆ‘ä»¬åªæƒ³ä¿å­˜æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±**model**çš„ç»“æ„è‡ªå®šä¹‰`save_model`æ–¹æ³•æ¥ä¿å­˜è‡ªå·±æƒ³ä¿å­˜çš„éƒ¨åˆ†ã€‚