# lightrainer
Simple Trainer for PyTorch Deep Learning

## ğŸ“˜ æ–‡æ¡£

<details open>
<summary>å®‰è£…</summary>

1. ç²¾ç®€ç‰ˆå®‰è£…
> å°†ä»“åº“å…‹éš†å¹¶åœ¨ä¸€ä¸ª[**Python â‰¥ 3.12.0**](https://www.python.org/)çš„ç¯å¢ƒä¸­å®‰è£…ä¾èµ–é¡¹ã€‚è¯·ç¡®ä¿ä½ å·²å®‰è£…[**PyTorch â‰¥ 2.7.0+cu128**](https://pytorch.org/get-started/locally/) ã€‚

```bash
git clone https://github.com/fordelkon/lightrainer.git

cd lightrainer

pip install -r requirements.txt
```

2. å®Œå¤‡ç‰ˆå®‰è£…ï¼ˆåœ¨ç²¾ç®€ç‰ˆçš„åŸºç¡€ä¸Šï¼‰
-  å®‰è£…[`pytorch_quantization`åº“](https://github.com/NVIDIA/TensorRT/blob/main/tools/pytorch-quantization/README.md)

- å®‰è£…[`wandb`åº“](https://pypi.org/project/wandb/)

-  å®‰è£…[`TensorRT`å‘½ä»¤è¡Œ](https://docs.nvidia.com/deeplearning/tensorrt/latest/installing-tensorrt/installing.html)
</details>

<details open>
<summary>ä»£ç è¯´æ˜</summary>

- **åŸºç±»è®­ç»ƒå™¨ï¼ˆBaseTranerï¼‰**
    - åŸºç±»è®­ç»ƒå™¨æä¾›è®­ç»ƒæ‰€éœ€è¦çš„åŸºæœ¬æ­¥éª¤: `train`ï¼Œ`validate`ï¼Œ`get_loss`ï¼Œ`get_loader`ï¼Œ`save_model` ç­‰æ–¹æ³•éœ€è¦æˆ‘ä»¬åœ¨ä¸åŒä»»åŠ¡ä¸­è‡ªå®šä¹‰éœ€æ±‚ï¼Œå…¶ä½™æ–¹æ³•æ¯”å¦‚ä¸€äº›æ˜¯æ¨¡å‹é‡åŒ–æ‰€éœ€è¦çš„æ–¹æ³•`replace_to_quant_modules`ã€`collect_stats`å’Œ`compute_max`ï¼Œè¿˜æœ‰ä¸€äº›æ˜¯æ— éœ€æ”¹åŠ¨çš„é…ç½®æ–¹æ³•ä»¥åŠä¼˜åŒ–æ–¹æ³•å¦‚`setup_train`ã€`build_optimizer`ã€`setup_scheduler`ã€`optimizer_step`ã€`get_memory`ã€`clear_memory`ï¼Œæœ€åä¸€äº›æ˜¯è®­ç»ƒåçš„ä»¥ä¸åŒçš„æ–‡ä»¶æ ¼å¼å¯¼å‡ºå¦‚`export_to_onnx`å’Œ`export_to_tensorrt`ã€‚

- **è®­ç»ƒå‚æ•°é…ç½®ï¼ˆYaMLæ–‡ä»¶ï¼‰**
    - è®­ç»ƒå‚æ•°å¯ä»¥é€šè¿‡è®¾è®¡ä¸åŒçš„`.yml`æ–‡ä»¶æ¥è¿›è¡Œé…ç½®ï¼Œæœ€åŸºæœ¬çš„å¯è®¾å®šå‚æ•°å¦‚ä¸‹
    ```yaml
    save_dir: runs_eeg  # (str) ç”¨äºä¿å­˜å®éªŒç»“æœçš„ç›®å½•
    device: 0           # (int) ä½¿ç”¨çš„è®¾å¤‡ç¼–å·ï¼Œä¾‹å¦‚ 0ã€1 æˆ– 2ï¼Œè¡¨ç¤ºä½¿ç”¨å“ªå— GPU
    batch: 32           # (int) æ‰¹é‡å¤§å°
    nbs: 64             # (int) æ ‡å‡†æ‰¹é‡å¤§å°ï¼ˆç”¨äºå½’ä¸€åŒ–å­¦ä¹ ç‡ï¼‰
    num_workers: 4      # (int) ç”¨äºå¹¶è¡ŒåŠ è½½æ•°æ®çš„å­è¿›ç¨‹æ•°é‡
    patience: 100       # (int) åœ¨æ²¡æœ‰æå‡çš„æƒ…å†µä¸‹ï¼Œæœ€å¤šç­‰å¾…å¤šå°‘ä¸ª epoch ååœæ­¢è®­ç»ƒ
    epochs: 200         # (int) æ€»è®­ç»ƒè½®æ•°
    exist_ok: True      # (bool) æ˜¯å¦å…è®¸è¦†ç›–å·²å­˜åœ¨çš„å®éªŒç›®å½•
    amp: False          # (bool) æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
    ptq: False          # (bool) æ˜¯å¦ä½¿ç”¨ NVIDIA çš„ pytorch_quantization è¿›è¡Œè®­ç»ƒåé‡åŒ–ï¼ˆPTQï¼‰
    num_batches: None   # (None æˆ– int) åœ¨ PTQ ä¸­ï¼Œç”¨äºæ”¶é›†ç»Ÿè®¡ä¿¡æ¯çš„æ‰¹æ¬¡æ•°é‡
    method: entropy     # (str) åœ¨ PTQ ä¸­ç”¨äºå†³å®šæœ€å¤§ç»å¯¹å€¼çš„æ–¹æ³•ï¼Œå¯é€‰å€¼=[entropy, max, percentile, mse]
    qat: False          # (bool) æ˜¯å¦ä½¿ç”¨ NVIDIA çš„ pytorch_quantization è¿›è¡Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰
    trt_quant: False    # (bool) æ˜¯å¦ä½¿ç”¨ TensorRT çš„ int8 é‡åŒ–

    optimizer: Adam     # (str) ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼Œå¯é€‰å€¼=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp]
    scheduler: Cosine   # (str) å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œå¯é€‰å€¼=[Cosine, Linear, Exponential, Polynomial]
    lr0: 0.01           # (float) åˆå§‹å­¦ä¹ ç‡ï¼ˆå¦‚ SGD=1E-2ï¼ŒAdam=1E-3ï¼‰
    lrf: 0.01           # (float) æœ€ç»ˆå­¦ä¹ ç‡ä¸ºåˆå§‹å­¦ä¹ ç‡ä¹˜ä»¥è¯¥å€¼
    gamma: 0.9          # (float) < 1.0 æ—¶ç”¨äºæŒ‡æ•°è¡°å‡çš„å› å­ï¼ˆæ¯è½®å­¦ä¹ ç‡ä¹˜ä»¥ gammaï¼‰
    power: 2.0          # (float) ç”¨äºå¤šé¡¹å¼è°ƒåº¦çš„æ›²çº¿å½¢çŠ¶æ§åˆ¶å› å­
    momentum: 0.937     # (float) SGD çš„åŠ¨é‡/Adam çš„ beta1 å‚æ•°
    weight_decay: 0.0005 # (float) ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡å‚æ•°ï¼Œå…¸å‹å€¼ä¸º 5e-4

    warmup_epochs: 3.0       # (float) é¢„çƒ­è®­ç»ƒçš„ epoch æ•°ï¼ˆæ”¯æŒå°æ•°ï¼‰
    warmup_momentum: 0.8     # (float) é¢„çƒ­é˜¶æ®µçš„åˆå§‹åŠ¨é‡
    warmup_bias_lr: 0.1      # (float) é¢„çƒ­é˜¶æ®µåç½®é¡¹çš„åˆå§‹å­¦ä¹ ç‡
    ```
    ä½†æ˜¯æˆ‘ä»¬ä»ç„¶å¯ä»¥ç»§ç»­åœ¨ä¸Šè¿°åŸºæœ¬é…ç½®å‚æ•°ä¸­ç»§ç»­æ·»åŠ ä¸€äº›ç‰¹å®šçš„å‚æ•°ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ã€‚æ¯”å¦‚è¯´åœ¨`VAVQE`çš„å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä¸Šè¿°å‚æ•°æ¡ä»¶ä¸‹æ·»åŠ ä»¥ä¸‹é¢å¤–å‚æ•°
    ```yaml
    recon_weight: 10.0
    lpips_weight: 1.0
    vq_weight: 0.5
    ```
    è¡¨ç¤ºå¯¹äºä¸åŒæŸå¤±æ‰€èµ‹äºˆçš„æƒé‡ï¼Œåœ¨è‡ªå®šä¹‰ä»»åŠ¡çš„è®­ç»ƒå™¨ä¸­è°ƒç”¨`self.kwdict[key]`æ¥è¿›è¡Œä½¿ç”¨ã€‚

- **åˆ†ç±»è®­ç»ƒå™¨ï¼ˆCLSTrainerï¼‰**
    - è‡ªå®šä¹‰åˆ†ç±»è®­ç»ƒå™¨ä¸­çš„è®­ç»ƒå‡½æ•°
    ```python 
    def train(self, use_wandb: bool=False):
        ......

        for epoch in range(1, self.epochs + 1):
            .......
            
            for i, batch in pbar:
                ......

                # Forward
                _, loss = self.get_loss(batch)
                self.train_loss += loss.item()
                
                # Backward
                self.scaler.scale(loss).backward()

                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()
                    last_opt_step = ni

                ......


            ......

            self.val_loss, self.fitness = self.validate()

            ......

    @torch.inference_mode()
    def validate(self):
        self.model.eval()
        pbar = enumerate(self.val_loader)
        pbar = tqdm(pbar, total=len(self.val_loader), bar_format="{l_bar}{bar:10}{r_bar}")
        y_true, y_pred = [], []

        val_loss = 0.0
        for _, batch in pbar:
            X, y = batch
            logits, loss = self.get_loss(batch)
            val_loss += loss.item()
            pred = logits.argmax(dim=1)
            y_true.append(y.cpu().numpy())
            y_pred.append(pred.cpu().numpy())
        
        val_loss /= len(self.val_loader)
        if self.use_wandb:
            wandb.log(
                {
                    "epoch": self.start_epoch,
                    "val/avg_loss": val_loss, 
                }
            )
        y_true = np.array(y_true).reshape(-1)
        y_pred = np.array(y_pred).reshape(-1)

        return val_loss, sklearn.metrics.accuracy_score(y_true, y_pred)
    
    def get_loss(self, batch):
        X, y = batch[0], batch[1]
        X, y = X.to(self.device), y.to(self.device)
        logits = self.model(X)
        loss = nn.functional.cross_entropy(logits, y) 
        return logits, loss
    ```

- **å›¾åƒç”Ÿæˆè®­ç»ƒå™¨ï¼ˆImGenTrainerï¼‰**
    - è‡ªå®šä¹‰å›¾åƒç”Ÿæˆè®­ç»ƒå™¨çš„è®­ç»ƒå‡½æ•°ï¼ˆé’ˆå¯¹`VQVAE`ï¼‰
    ```python
    def train(self, use_wandb: bool=False):
        ......

        for epoch in range(1, self.epochs + 1):
            ......
            
            for i, batch in pbar:
                ......

                # Forward
                loss_vq, loss_recon, loss_lpips, perplexity, psnr, ssim = self.get_loss(batch)
                loss = self.kwdict["vq_weight"] * loss_vq + self.kwdict["recon_weight"] * loss_recon + self.kwdict["lpips_weight"] * loss_lpips
                self.train_loss += loss.item()
                self.train_loss_recon += loss_recon.item()
                self.train_loss_lpips += loss_lpips.item()
                self.train_loss_vq += loss_vq.item()
                self.train_perplexity += perplexity.item()
                
                # Backward
                self.scaler.scale(loss).backward()

                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()
                    last_opt_step = ni

                ......

            ......

            self.train_loss /= nb
            self.train_loss_recon /= nb
            self.train_loss_lpips /= nb
            self.train_loss_vq /= nb
            self.train_perplexity /= nb

            ......

            self.val_loss, perplexity, self.fitness, ssim= self.validate()

            ......

    @torch.inference_mode()
    def validate(self):
        self.model.eval()
        pbar = enumerate(self.val_loader)
        pbar = tqdm(pbar, total=len(self.val_loader), bar_format="{l_bar}{bar:10}{r_bar}")

        val_loss = 0.
        val_loss_recon = 0.
        val_loss_lpips = 0.
        val_loss_vq = 0.
        val_perplexity = 0.
        val_ssim = 0.
        val_psnr = 0.
        nb_val = len(self.val_loader)
        for _, batch in pbar:
            loss_vq, loss_recon, loss_lpips, perplexity, psnr, ssim = self.get_loss(batch)
            loss = self.kwdict["vq_weight"] * loss_vq + self.kwdict["recon_weight"] * loss_recon + self.kwdict["lpips_weight"] * loss_lpips
            val_loss += loss.item()
            val_loss_recon += loss_recon.item()
            val_loss_lpips += loss_lpips.item()
            val_loss_vq += loss_vq.item()
            val_perplexity += perplexity.item()
            val_ssim += ssim
            val_psnr += psnr
        val_loss /= nb_val
        val_loss_recon /= nb_val
        val_loss_lpips /= nb_val
        val_loss_vq /= nb_val
        val_perplexity /= nb_val
        val_ssim /= nb_val
        val_psnr /= nb_val

        if self.use_wandb:
            wandb.log(
                {
                    "epoch": self.start_epoch,
                    "val/avg_loss": val_loss, 
                    "val/avg_recon_loss": val_loss_recon, 
                    "val/avg_perceptual_loss": val_loss_lpips, 
                    "val/avg_vq_loss": val_loss_vq, 
                    "val/avg_psnr": val_psnr, 
                    "val/avg_ssim": val_ssim, 
                    "val/avg_perplexity": val_perplexity,
                }
            )

        return val_loss, val_perplexity, val_psnr, val_ssim
    
    def get_loss(self, batch):
        X = batch[0]
        X = X.to(self.device)
        X_recon, loss_vq, perplexity = self.model(X)
        # Reconstruction Loss
        loss_recon = nn.functional.mse_loss(X_recon, X)
        # LPIS Loss
        X_norm, X_recon_norm = X * 2. - 1., X_recon * 2. - 1.
        loss_lpips = self.lpips_fn(X_norm, X_recon_norm).mean()

        psnr = kornia.metrics.psnr(X, X_recon, max_val=1.)
        
        ssim = kornia.metrics.ssim(X, X_recon, window_size=11).mean()
    
        return (loss_vq, loss_recon, loss_lpips, 
                perplexity, psnr, ssim)
    ```
</details>

<details open>
<summary>æ€»ç»“</summary>

å¯¹æ¯”ä¸¤ä¸ªç‰¹å®šä»»åŠ¡çš„`train`æ–¹æ³•å¯ä»¥çœ‹å‡ºï¼Œè¯¥æ–¹æ³•çš„åŸºæœ¬æ¡†æ¶æ˜¯æ²¡æœ‰ä»€ä¹ˆå˜åŒ–çš„ï¼Œä¸»è¦å˜åŒ–çš„æ˜¯`get_loss`å’Œ`validate`æ–¹æ³•çš„è¿”å›ç»“æœï¼Œç”±äºä¸åŒæ¨¡å‹çš„æŸå¤±ä»¥åŠé€‰æ‹©ä¿å­˜æ¨¡å‹çš„è¯„åˆ¤æ ‡å‡†ä¸åŒï¼Œæ‰€ä»¥è¿™ä¸¤ä¸ªæ–¹æ³•å…·æœ‰å¾ˆå¼ºçš„çµæ´»æ€§ã€‚å½“ç„¶æœ‰äº›æ—¶å€™éœ€è¦å¯¹`dataset`è¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†æ¯”å¦‚`padding`ç­‰æ–¹æ³•ï¼Œéœ€è¦å†å®šä¹‰`collate_fn`æ–¹æ³•å†`get_loader`é‡Œä½¿ç”¨ã€‚æœ‰äº›æƒ…å†µæˆ‘ä»¬åªæƒ³ä¿å­˜æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±**model**çš„ç»“æ„è‡ªå®šä¹‰`save_model`æ–¹æ³•æ¥ä¿å­˜è‡ªå·±æƒ³ä¿å­˜çš„éƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯è¯´åç»­å¯ä»¥æ ¹æ®ä¸Šè¿°ä¸¤ç§èŒƒä¾‹ç»§ç»­å®šä¹‰æ›´å¤šé’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„è®­ç»ƒå™¨ã€‚
</details>

<details open>
<summary>ä½¿ç”¨æ•™ç¨‹</summary>

- é¡¹ç›®ç›®å½•ç»“æ„
<pre lang="markdown">
```
ğŸ“ lightrainer
.
â”œâ”€â”€ cls_trainer.py              # åˆ†ç±»è®­ç»ƒå™¨
â”œâ”€â”€ imgen_trainer.py            # å›¾åƒç”Ÿæˆè®­ç»ƒå™¨
â”œâ”€â”€ modules.py                  # æ¨¡å‹ç»“æ„æˆ–æ¨¡å—å®šä¹‰
â”œâ”€â”€ README.md                   # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ requirements.txt            # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ trainer_cls_cfg.yml         # åˆ†ç±»ä»»åŠ¡é…ç½®æ–‡ä»¶
â”œâ”€â”€ trainer_imgen_cfg.yml       # å›¾åƒç”Ÿæˆä»»åŠ¡é…ç½®æ–‡ä»¶
â”œâ”€â”€ trainer.py                  # åŸºç±»è®­ç»ƒå™¨
â””â”€â”€ utils.py                    # å·¥å…·å‡½æ•°
```
</pre>

- å¤šæ¨¡æ€å›¾æ¨¡å‹èŠ‚ç‚¹åˆ†ç±»ä»»åŠ¡ï¼ˆä»¥`MMGCN`ä¸ºä¾‹ï¼Œ[colab notebooké“¾æ¥](https://colab.research.google.com/drive/1cWtSiUQm0J7kUJQyveMcdxTXJF8L5jT0?usp=sharing)ï¼‰
    - å°†`lightrainer`é¡¹ç›®å…‹éš†ä¸‹æ¥ä¹‹åæ–°å»ºç«‹è¦è®­ç»ƒçš„`MMGCN`æ¨¡å‹æ–‡ä»¶ä¸è¯¥é¡¹ç›®åŒçº§ï¼ˆæ–‡ä»¶æ’å¸ƒå¦‚ä¸‹æ‰€ç¤ºï¼‰
<pre lang="markdown">
ğŸ“¦ myprojects
â”œâ”€â”€ ğŸ“ lightrainer 
â”œâ”€â”€ IEMOCAP_features.pkl
â””â”€â”€ mmgcn.ipynb # MMGCNåˆ†ç±»
</pre>

- å›¾åƒç”Ÿæˆä»»åŠ¡ï¼ˆä»¥`VQVAE`ä¸ºä¾‹ï¼‰
    - å°†`lightrainer`é¡¹ç›®å…‹éš†ä¸‹æ¥ä¹‹åæ–°å»ºç«‹è¦è®­ç»ƒçš„``æ¨¡å‹æ–‡ä»¶ä¸è¯¥é¡¹ç›®åŒçº§ï¼ˆæ–‡ä»¶æ’å¸ƒå¦‚ä¸‹æ‰€ç¤ºï¼‰
<pre lang="markdown">
ğŸ“¦ myprojects
â”œâ”€â”€ ğŸ“ lightrainer 
â””â”€â”€ vqvae.ipynb # å¤šæ¨¡æ€ä½ç§©èåˆåˆ†ç±»  
</pre>
</details>